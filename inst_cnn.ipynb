{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5jC20g1r-Jba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import lzma\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensors = []\n",
        "images = []\n",
        "directory = 'bzzdwn'\n",
        "size = 1440, 1440\n",
        "\n",
        "path = Path(directory)\n",
        "for file in sorted(path.iterdir()):\n",
        "    if file.is_file() and file.suffix == '.jpg':\n",
        "        image = Image.open(file)\n",
        "        #image.thumbnail(size, Image.Resampling.LANCZOS)\n",
        "        images.append(image)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "for image in images:\n",
        "    img_tensors.append(TF.to_tensor(image))"
      ],
      "metadata": {
        "id": "2TvKvET--6Ma"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_jsonl_xz(file_path):\n",
        "  with lzma.open(file_path, 'rt', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      yield json.loads(line)"
      ],
      "metadata": {
        "id": "-e_TfaPSJqXT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "likes = []\n",
        "\n",
        "for file in sorted(path.iterdir()):\n",
        "    if file.is_file() and file.suffix == '.xz':\n",
        "        data = list(read_jsonl_xz(file))\n",
        "        data = pd.DataFrame(data[0])\n",
        "        #data = pd.read_json(file)\n",
        "        likes.append(data.loc['edge_media_preview_like'].node['count'])"
      ],
      "metadata": {
        "id": "87YLcLCF-6n4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_height = max([img.size(1) for img in img_tensors])\n",
        "max_width = max([img.size(2) for img in img_tensors])\n",
        "\n",
        "img_tensors = [\n",
        "    F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n",
        "    for img in img_tensors\n",
        "]"
      ],
      "metadata": {
        "id": "AhN1KluN--Nn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensors = torch.stack(img_tensors)"
      ],
      "metadata": {
        "id": "9Fb1mdxV_Ep9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps18Fhjp_KlH",
        "outputId": "89b7712f-424c-48e9-a02c-c0d52c9b3abf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 3, 1484, 1440])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = list(zip(img_tensors, likes))\n",
        "random.shuffle(temp)\n",
        "X, y = zip(*temp)\n",
        "X, y = list(X), list(y)\n",
        "X_train, y_train = X[:11], y[:11]\n",
        "X_test, y_test = X[11], y[11]"
      ],
      "metadata": {
        "id": "MaSbHeLVZ2sb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    This will be the very basic CNN model we will use for the regression task.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.conv1 = nn.Conv2d(in_channels=self.image_size[0], out_channels=4, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.linear_line_size = int(16*(image_size[1]//4)*(image_size[2]//4))\n",
        "        self.fc1 = nn.Linear(in_features=self.linear_line_size, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Passes the data through the network.\n",
        "        There are commented out print statements that can be used to\n",
        "        check the size of the tensor at each layer. These are very useful when\n",
        "        the image size changes and you want to check that the network layers are\n",
        "        still the correct shape.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        #print('Size of tensor after each layer')\n",
        "        #print(f'conv1 {x.size()}')\n",
        "        x = nn.functional.relu(x)\n",
        "        #print(f'relu1 {x.size()}')\n",
        "        x = self.pool1(x)\n",
        "        #print(f'pool1 {x.size()}')\n",
        "        x = self.conv2(x)\n",
        "        #print(f'conv2 {x.size()}')\n",
        "        x = nn.functional.relu(x)\n",
        "        #print(f'relu2 {x.size()}')\n",
        "        x = self.pool2(x)\n",
        "        #print(f'pool2 {x.size()}')\n",
        "        x = x.view(-1, self.linear_line_size)\n",
        "        #print(f'view1 {x.size()}')\n",
        "        x = self.fc1(x)\n",
        "        #print(f'fc1 {x.size()}')\n",
        "        x = nn.functional.relu(x)\n",
        "        #print(f'relu2 {x.size()}')\n",
        "        x = self.fc2(x)\n",
        "        #print(f'fc2 {x.size()}')\n",
        "        return x"
      ],
      "metadata": {
        "id": "ovEvyqXU_NtL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.FloatTensor(y_train)"
      ],
      "metadata": {
        "id": "6peDHHsOAcv3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [(X_train[i], y_train[i]) for i in range(0, len(y_train))]"
      ],
      "metadata": {
        "id": "POnjculq_s1t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN([3, 1484, 1440])\n",
        "print(model)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (inputs, targets) in enumerate(dataset):\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # Print training statistics\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i + 1}/{len(dataset)}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEFBivwk_RFu",
        "outputId": "628298cd-4bb2-4bba-9024-b05309641e1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2136960, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/11], Loss: 261.1115\n",
            "Epoch [2/10], Step [10/11], Loss: 751.5861\n",
            "Epoch [3/10], Step [10/11], Loss: 2.2399\n",
            "Epoch [4/10], Step [10/11], Loss: 644.8683\n",
            "Epoch [5/10], Step [10/11], Loss: 17.1728\n",
            "Epoch [6/10], Step [10/11], Loss: 0.9448\n",
            "Epoch [7/10], Step [10/11], Loss: 32.1507\n",
            "Epoch [8/10], Step [10/11], Loss: 13.9883\n",
            "Epoch [9/10], Step [10/11], Loss: 0.0033\n",
            "Epoch [10/10], Step [10/11], Loss: 99.4594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtbdI8XMLrIh",
        "outputId": "50454a5a-535d-4f96-f3c0-f4ec5cc410a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[61.8760]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWqJfB_DQZ8s",
        "outputId": "17201b01-b03e-433c-e082-508b4751ef14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2T8CpZMQr8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}